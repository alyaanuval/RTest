summvar
# Correlations (x1,y1),(x1,y2),(x1,y3),(x2,y4)
cor.AD <- cor(AnscombesData[, c("x1", "y1", "y2", "y3", "x2", "y4")], use="complete.obs")
cor.AD["x1","y1"]
cor.AD["x1","y2"]
cor.AD["x1","y3"]
cor.AD["x2","y4"]
# Linear regression
model1 <- lm(y1~x1)
model2 <- lm(y2~x1)
model3 <- lm(y3~x1)
model4 <- lm(y4~x2)
# Table of regression estimates for each model
name <- c("(x1,y1)","(x1,y2)","(x1,y3)","(x2,y4)")
intercept <- c(model1$coefficients[1], model2$coefficients[1],
model3$coefficients[1], model4$coefficients[1])
slope <- c(model1$coefficients[2], model2$coefficients[2],
model3$coefficients[2], model4$coefficients[2])
data.frame(name, intercept, slope)
# Regression plots
par(mfrow=c(2, 2))
plot(y1~x1, xlab="x1", ylab="y1", xlim=c(0,20), ylim=c(0,15), main="Scatter Plot of y1 vs x1")
abline(model1, col="blue")
plot(y2~x1, xlab="x1", ylab="y2", xlim=c(0,20), ylim=c(0,15), main="Scatter Plot of y2 vs x1")
abline(model1, col="red")
plot(y3~x1, xlab="x1", ylab="y3", xlim=c(0,20), ylim=c(0,15), main="Scatter Plot of y3 vs x1")
abline(model3, col="orange")
plot(y4~x2, xlab="x2", ylab="y4", xlim=c(0,20), ylim=c(0,15), main="Scatter Plot of y4 vs x2")
abline(model4, col="green")
# Clear workspace
rm(list=ls())
# Read in required dataset
AnscombesData <- read.csv("~/ActSci654/AnscombesData.csv")
# Verify data
AnscombesData
# Useful variables
x1 <- AnscombesData$x1
y1 <- AnscombesData$y1
y2 <- AnscombesData$y2
y3 <- AnscombesData$y3
x2 <- AnscombesData$x2
y4 <- AnscombesData$y4
# Find mean and standard deviation of data
Xymat <- data.frame(x1, y1, y2, y3, x2, y4)
meanSummary <- sapply(Xymat, mean,  na.rm=TRUE)
sdSummary   <- sapply(Xymat, sd,    na.rm=TRUE)
summvar = cbind(meanSummary, sdSummary)
summvar
# Correlations (x1,y1),(x1,y2),(x1,y3),(x2,y4)
cor.AD <- cor(AnscombesData[, c("x1", "y1", "y2", "y3", "x2", "y4")], use="complete.obs")
cor.AD["x1","y1"]
cor.AD["x1","y2"]
cor.AD["x1","y3"]
cor.AD["x2","y4"]
# Linear regression
model1 <- lm(y1~x1)
model2 <- lm(y2~x1)
model3 <- lm(y3~x1)
model4 <- lm(y4~x2)
# Table of regression estimates for each model
name <- c("(x1,y1)","(x1,y2)","(x1,y3)","(x2,y4)")
intercept <- c(model1$coefficients[1], model2$coefficients[1],
model3$coefficients[1], model4$coefficients[1])
slope <- c(model1$coefficients[2], model2$coefficients[2],
model3$coefficients[2], model4$coefficients[2])
data.frame(name, intercept, slope)
# Scatter plots and regression lines
par(mfrow=c(2, 2))
plot(y1~x1, xlab="x1", ylab="y1", xlim=c(0,20), ylim=c(0,15), main="Scatter Plot of y1 vs x1")
abline(model1, col="blue")
plot(y2~x1, xlab="x1", ylab="y2", xlim=c(0,20), ylim=c(0,15), main="Scatter Plot of y2 vs x1")
abline(model1, col="red")
plot(y3~x1, xlab="x1", ylab="y3", xlim=c(0,20), ylim=c(0,15), main="Scatter Plot of y3 vs x1")
abline(model3, col="orange")
plot(y4~x2, xlab="x2", ylab="y4", xlim=c(0,20), ylim=c(0,15), main="Scatter Plot of y4 vs x2")
abline(model4, col="green")
# Residual plots
par(mfrow=c(2, 2))
names(model1)
qqnorm(model1$residuals)
qqline(model1$residuals, col = "blue")
names(model2)
qqnorm(model2$residuals)
qqline(model2$residuals, col = "red")
names(model3)
qqnorm(model3$residuals)
qqline(model3$residuals, col = "orange")
names(model4)
qqnorm(model4$residuals)
qqline(model4$residuals, col = "green")
par(mfrow = c(2,2))
plot(model1)
par(mfrow = c(2,2))
plot(model1, col='blue')
par(mfrow = c(2,2))
plot(model1, col="blue")
# Clear workspace
rm(list=ls())
# Read in required dataset
AnscombesData <- read.csv("~/ActSci654/AnscombesData.csv")
# Verify data
AnscombesData
# Useful variables
x1 <- AnscombesData$x1
y1 <- AnscombesData$y1
y2 <- AnscombesData$y2
y3 <- AnscombesData$y3
x2 <- AnscombesData$x2
y4 <- AnscombesData$y4
# Find mean and standard deviation of data
Xymat <- data.frame(x1, y1, y2, y3, x2, y4)
meanSummary <- sapply(Xymat, mean,  na.rm=TRUE)
sdSummary   <- sapply(Xymat, sd,    na.rm=TRUE)
summvar = cbind(meanSummary, sdSummary)
summvar
# Correlations (x1,y1),(x1,y2),(x1,y3),(x2,y4)
cor.AD <- cor(AnscombesData[, c("x1", "y1", "y2", "y3", "x2", "y4")], use="complete.obs")
cor.AD["x1","y1"]
cor.AD["x1","y2"]
cor.AD["x1","y3"]
cor.AD["x2","y4"]
# Linear regression
model1 <- lm(y1~x1)
model2 <- lm(y2~x1)
model3 <- lm(y3~x1)
model4 <- lm(y4~x2)
# Table of regression estimates for each model
name <- c("(x1,y1)","(x1,y2)","(x1,y3)","(x2,y4)")
intercept <- c(model1$coefficients[1], model2$coefficients[1],
model3$coefficients[1], model4$coefficients[1])
slope <- c(model1$coefficients[2], model2$coefficients[2],
model3$coefficients[2], model4$coefficients[2])
data.frame(name, intercept, slope)
# Scatter plots and regression lines
par(mfrow=c(2, 2))
plot(y1~x1, xlab="x1", ylab="y1", xlim=c(0,20), ylim=c(0,15), main="Scatter Plot of y1 vs x1")
abline(model1, col="blue")
plot(y2~x1, xlab="x1", ylab="y2", xlim=c(0,20), ylim=c(0,15), main="Scatter Plot of y2 vs x1")
abline(model1, col="red")
plot(y3~x1, xlab="x1", ylab="y3", xlim=c(0,20), ylim=c(0,15), main="Scatter Plot of y3 vs x1")
abline(model3, col="orange")
plot(y4~x2, xlab="x2", ylab="y4", xlim=c(0,20), ylim=c(0,15), main="Scatter Plot of y4 vs x2")
abline(model4, col="green")
# Residual plots
par(mfrow=c(2, 2))
names(model1)
qqnorm(model1$residuals)
qqline(model1$residuals, col = "blue")
names(model2)
qqnorm(model2$residuals)
qqline(model2$residuals, col = "red")
names(model3)
qqnorm(model3$residuals)
qqline(model3$residuals, col = "orange")
names(model4)
qqnorm(model4$residuals)
qqline(model4$residuals, col = "green")
# Alternative method to form residual plots
par(mfrow = c(2,2))
plot(model1, col="blue")
par(mfrow = c(2,2))
plot(model1, col="red")
par(mfrow = c(2,2))
plot(model1, col="orange")
par(mfrow = c(2,2))
plot(model1, col="green")
# Linear regression
model1 <- lm(y1~x1)
summary(model1)
model2 <- lm(y2~x1)
summary(model2)
model3 <- lm(y3~x1)
summary(model3)
model4 <- lm(y4~x2)
summary(model4)
n <- 40
k <- 5
R.2 <- 0.20
F.ratio <- (R.2/(1-R.2))*((n-(k+1))/k)
F.ratio
n <- 40
k <- 5
R.2 <- 0.20
F.ratio <- function (n, k, R.2)
{
(R.2/(1-R.2))*((n-(k+1))/k)
}
F.ratio (40,5,0.20)
n <- 40
k <- 5
R.2 <- 0.20
F.ratio <- function (n, k, R.2)
{
(R.2/(1-R.2))*((n-(k+1))/k)
}
F.ratio (40,5,0.20)
F.ratio (400,5,0.20)
# n <- 40
# k <- 5
# R.2 <- 0.20
F.ratio <- function (n, k, R.2)
{
(R.2/(1-R.2))*((n-(k+1))/k)
}
F.ratio (40,5,0.20)
F.ratio (400,5,0.20)
F.ratio <- function (n, k, R.2)
{
(R.2/(1-R.2))*((n-(k+1))/k)
}
F.ratio (40,5,0.20)
F.ratio (400,5,0.20)
F.ratio <- function (n, k, R.2)
{
(R.2/(1-R.2))*((n-(k+1))/k)
}
F.ratio (n=40,k=5,R.2=0.20)
F.ratio (n=400,k=5,R.2=0.20)
# F.ratio <- function (n, k, R.2)
# {
#   (R.2/(1-R.2))*((n-(k+1))/k)
# }
#
# F.ratio (n=40,k=5,R.2=0.20)
#
# F.ratio (n=400,k=5,R.2=0.20)
HC <- read.csv("~/ActSci654/HospitalCosts.csv")
# Read in required data set
HC <- read.csv("~/ActSci654/HospitalCosts.csv")
# Useful variables
LNTOTCHG <- log(HC$TOTCHG)
AGE <- HC$AGE
# Scatter plot of LNTOTCHG vs AGE
plot(LNTOTCHG~AGE)
# Read in required data set
HC <- read.csv("~/ActSci654/HospitalCosts.csv")
# Useful variables
LNTOTCHG <- log(HC$TOTCHG)
AGE <- HC$AGE
# Scatter plot of LNTOTCHG vs AGE
plot(LNTOTCHG~AGE)
# Correlation of LNTOTCHG and AGE
cor(LNTOTCHG, AGE)
# Read in required data set
HC <- read.csv("~/ActSci654/HospitalCosts.csv")
# Useful variables
LNTOTCHG <- log(HC$TOTCHG)
AGE <- HC$AGE
# Scatter plot of LNTOTCHG vs AGE
plot(LNTOTCHG~AGE)
# Correlation of LNTOTCHG and AGE
cor(LNTOTCHG, AGE)
# Linear regression of LNTOTCHG and AGE
regMod <- lm(LNTOTCHG~AGE)
summary(regMod)
# Make AGE a binary variable
HC$AGE <- with(HC, 1*(AGE == 0))
# Read in required data set
HC <- read.csv("~/ActSci654/HospitalCosts.csv")
# Useful variables
LNTOTCHG <- log(HC$TOTCHG)
AGE <- HC$AGE
# Scatter plot of LNTOTCHG vs AGE
plot(LNTOTCHG~AGE)
# Correlation of LNTOTCHG and AGE
cor(LNTOTCHG, AGE)
# Linear regression of LNTOTCHG and AGE
regMod <- lm(LNTOTCHG~AGE)
summary(regMod)
# Make AGE a binary variable (1 if age is 0, 0 otherwise)
HC$NB <- with(HC, 1*(AGE == 0))
# Read in required data set
HC <- read.csv("~/ActSci654/HospitalCosts.csv")
# Useful variables
LNTOTCHG <- log(HC$TOTCHG)
AGE <- HC$AGE
# Scatter plot of LNTOTCHG vs AGE
plot(LNTOTCHG~AGE)
# Correlation of LNTOTCHG and AGE
cor(LNTOTCHG, AGE)
# Linear regression of LNTOTCHG and AGE
regMod <- lm(LNTOTCHG~AGE)
summary(regMod)
# Make AGE a binary variable (1 if age is 0, 0 otherwise)
HC$NB <- with(HC, 1*(AGE == 0))
# Linear regression of LNTOTCHG and NB, AGE
regMod2 <- lm(LNTOTCHG~NB + AGE)
summary(regMod2)
# Read in required data set
HC <- read.csv("~/ActSci654/HospitalCosts.csv")
# Useful variables
LNTOTCHG <- log(HC$TOTCHG)
AGE <- HC$AGE
# Scatter plot of LNTOTCHG vs AGE
plot(LNTOTCHG~AGE)
# Correlation of LNTOTCHG and AGE
cor(LNTOTCHG, AGE)
# Linear regression of LNTOTCHG and AGE
regMod <- lm(LNTOTCHG~AGE)
summary(regMod)
# Make AGE a binary variable (1 if age is 0, 0 otherwise)
HC$NB <- with(HC, 1*(AGE == 0))
NB <- HC$NB
# Linear regression of LNTOTCHG and NB, AGE
regMod2 <- lm(LNTOTCHG~NB+AGE)
summary(regMod2)
HC$NB <- with(HC, 1*(AGE == 0))
Check1 <- data.frame(AGE, HC$NB)
fix(Check1)
regMod2 <- lm(LNTOTCHG~NB+AGE)
summary(regMod2)
# Clear workspace
rm(list=ls())
# Read in required data set
HC <- read.csv("~/ActSci654/HospitalCosts.csv")
# Useful variables
LNTOTCHG <- log(HC$TOTCHG)
AGE <- HC$AGE
# Scatter plot of LNTOTCHG vs AGE
plot(LNTOTCHG~AGE)
# Correlation of LNTOTCHG and AGE
cor(LNTOTCHG, AGE)
# Linear regression of LNTOTCHG and AGE
regMod <- lm(LNTOTCHG~AGE)
summary(regMod)
# Make AGE a binary variable (1 if age is 0, 0 otherwise)
HC$NB <- with(HC, 1*(AGE == 0))
Check1 <- data.frame(AGE, HC$NB)
fix(Check1)
# Linear regression of LNTOTCHG and NB, AGE
regMod2 <- lm(LNTOTCHG~NB+AGE)
summary(regMod2)
# Make AGE a binary variable (1 if age is 0, 0 otherwise)
HC$NB <- with(HC, 1*(AGE == 0))
Check1 <- data.frame(AGE, HC$NB)
fix(Check1)
NB <- HC$NB
# Linear regression of LNTOTCHG and NB, AGE
regMod2 <- lm(LNTOTCHG~AGE+NB)
summary(regMod2)
# Read in required data set
HC <- read.csv("~/ActSci654/HospitalCosts.csv")
# Useful variables
LNTOTCHG <- log(HC$TOTCHG)
AGE <- HC$AGE
# Scatter plot of LNTOTCHG vs AGE
plot(LNTOTCHG~AGE)
# Correlation of LNTOTCHG and AGE
cor(LNTOTCHG, AGE)
# Linear regression of LNTOTCHG and AGE
regMod <- lm(LNTOTCHG~AGE)
summary(regMod)
# Make AGE a binary variable (1 if age is 0, 0 otherwise)
HC$NB <- with(HC, 1*(AGE == 0))
Check1 <- data.frame(AGE, HC$NB)
fix(Check1)
NB <- HC$NB
# Linear regression of LNTOTCHG and NB, AGE
regMod2 <- lm(LNTOTCHG~AGE+NB)
# ANOVA F-test
summary(regMod2)
anova(regMod2)
aov(regMod2)
# Read in required data set
HC <- read.csv("~/ActSci654/HospitalCosts.csv")
# Useful variables
LNTOTCHG <- log(HC$TOTCHG)
AGE <- HC$AGE
FEMALE <- HC$FEMALE
# Scatter plot of LNTOTCHG vs AGE
plot(LNTOTCHG~AGE)
# Correlation of LNTOTCHG and AGE
cor(LNTOTCHG, AGE)
# Linear regression of LNTOTCHG and AGE
regMod <- lm(LNTOTCHG~AGE)
summary(regMod)
# Make AGE a binary variable (1 if age is 0, 0 otherwise)
HC$NB <- with(HC, 1*(AGE == 0))
Check1 <- data.frame(AGE, HC$NB)
fix(Check1)
NB <- HC$NB
# Linear regression of LNTOTCHG and AGE, NB
regMod2 <- lm(LNTOTCHG~AGE+NB)
# ANOVA F-test
summary(regMod2)
anova(regMod2)
aov(regMod2)
# Yes, the binary variable is statistically significant. From the ANOVA table, we can see that the p-value(Pr(>F)) is small suggesting that we should reject the null hypothesis that beta2 is 0.
# Examine sex effect
# Linear regression of LNTOTCHG and AGE, FEMALE
regMod3 <- lm(LNTOTCHG~AGE+FEMALE)
# ANOVA F-test
summary(regMod3)
anova(regMod3)
aov(regMod3)
# Read in required data set
HC <- read.csv("~/ActSci654/HospitalCosts.csv")
# Useful variables
LNTOTCHG <- log(HC$TOTCHG)
AGE <- HC$AGE
FEMALE <- HC$FEMALE
# Scatter plot of LNTOTCHG vs AGE
plot(LNTOTCHG~AGE)
# Correlation of LNTOTCHG and AGE
cor(LNTOTCHG, AGE)
# Linear regression of LNTOTCHG and AGE
regMod <- lm(LNTOTCHG~AGE)
summary(regMod)
# Make AGE a binary variable (1 if age is 0, 0 otherwise)
HC$NB <- with(HC, 1*(AGE == 0))
Check1 <- data.frame(AGE, HC$NB)
fix(Check1)
NB <- HC$NB
# Linear regression of LNTOTCHG and AGE, NB
regMod2 <- lm(LNTOTCHG~AGE+NB)
# ANOVA F-test
summary(regMod2)
anova(regMod2)
aov(regMod2)
# Yes, the binary variable is statistically significant. From the ANOVA table, we can see that the p-value(Pr(>F)) is small suggesting that we should reject the null hypothesis that beta2 is 0.
# Examine sex effect
# Linear regression of LNTOTCHG and AGE, FEMALE
regMod3 <- lm(LNTOTCHG~AGE+FEMALE)
# ANOVA F-test
summary(regMod3)
anova(regMod3)
aov(regMod3)
# Yes, FEMALE is statistically significant. From the ANOVA table, we can see that the p-value(Pr(>F)) is small suggesting that we should reject the null hypothesis that beta2 is 0.
# Examine interaction between AGE and FEMALE
# Linear regression of LNTOTCHG and AGE, FEMALE
regMod4 <- lm(LNTOTCHG~AGE+FEMALE+(AGE*FEMALE))
# ANOVA F-test
summary(regMod4)
anova(regMod4)
aov(regMod4)
anova(regMod4)
summary(regMod4)
summary(regMod4)
anova(regMod4)
qf(.95, df1=k, df2=n-(k+1))   #critical F-value
qf(.95, df1=5, df2=34)
40-(6)
400-(6)
qf(.95, df1=5, df2=394)
F.ratio (n=40,k=5,R.2=0.20)
qf(.95, df1=5, df2=34)   #critical F-value
# e.
F.ratio (n=400,k=5,R.2=0.20)
qf(.95, df1=5, df2=394)
F.ratio <- function (n, k, R.2)
{
(R.2/(1-R.2))*((n-(k+1))/k)
}
# d.
F.ratio (n=40,k=5,R.2=0.20)
qf(.95, df1=5, df2=34)   #critical F-value
# e.
F.ratio (n=400,k=5,R.2=0.20)
qf(.95, df1=5, df2=394)
F.ratio <- function (n, k, R.2)
{
(R.2/(1-R.2))*((n-(k+1))/k)
}
# d.
F.ratio (n=40,k=5,R.2=0.20)
qf(.95, df1=5, df2=34)   #critical F-value
# e.
F.ratio (n=400,k=5,R.2=0.20)
qf(.95, df1=5, df2=394)   #critical F-value
# Read in required data set
HC <- read.csv("~/ActSci654/HospitalCosts.csv")
# Useful variables
LNTOTCHG <- log(HC$TOTCHG)
AGE <- HC$AGE
FEMALE <- HC$FEMALE
# Scatter plot of LNTOTCHG vs AGE
plot(LNTOTCHG~AGE)
# Correlation of LNTOTCHG and AGE
cor(LNTOTCHG, AGE)
# Linear regression of LNTOTCHG and AGE
regMod <- lm(LNTOTCHG~AGE)
summary(regMod)
# Make AGE a binary variable (1 if age is 0, 0 otherwise)
HC$NB <- with(HC, 1*(AGE == 0))
Check1 <- data.frame(AGE, HC$NB)
fix(Check1)
NB <- HC$NB
# Linear regression of LNTOTCHG and AGE, NB
regMod2 <- lm(LNTOTCHG~AGE+NB)
# ANOVA F-test
summary(regMod2)
anova(regMod2)
aov(regMod2)
# Yes, the binary variable is statistically significant. From the ANOVA table, we can see that the p-value(Pr(>F)) is small suggesting that we should reject the null hypothesis that beta2 is 0.
# Examine sex effect
# Linear regression of LNTOTCHG and AGE, FEMALE
regMod3 <- lm(LNTOTCHG~AGE+FEMALE)
# ANOVA F-test
summary(regMod3)
anova(regMod3)
aov(regMod3)
# Yes, FEMALE is statistically significant. From the ANOVA table, we can see that the p-value(Pr(>F)) is small suggesting that we should reject the null hypothesis that beta2 is 0.
# Examine interaction between AGE and FEMALE
# Linear regression of LNTOTCHG and AGE, FEMALE
regMod4 <- lm(LNTOTCHG~AGE+FEMALE+(AGE*FEMALE))
# ANOVA F-test
summary(regMod4)
anova(regMod4)
aov(regMod4)
# Yes, FEMALE is statistically significant. From the ANOVA table, we can see that the p-value(Pr(>F)) is small suggesting that we should reject the null hypothesis that beta2 is 0.
anova(regMod2)
install.packages("goftest")
